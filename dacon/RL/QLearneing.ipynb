{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff83db0-5645-435a-b40f-d3750684a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddd4ef-aeba-4d73-925a-a00e2ea5d503",
   "metadata": {},
   "source": [
    "# 1번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe15e7f-7ede-423f-81a2-f8c9267d6b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, actions):\n",
    "        # 행동  = [0,1,2,3] 순서대로 상하좌우\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 0.01\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.9\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n",
    "        \n",
    "    # <s, a, r, s'> 샘플로부터 큐함수 업데이트\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        q_1 = self.q_table[state][action]\n",
    "        # 벨만 최적 방정식을 사용한 큐함수의 업데이트\n",
    "        q_2 = reward + self.discount_factor * max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.learning_rate * (q_2 - q_1)\n",
    "\n",
    "    # 큐함수에 의거하여 입실론 탐욕 정책에 따라서 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            # 무작위 행동 반환\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # 큐함수에 따른 행동 반환\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210f6fc-10db-4425-bc22-da9e58c75922",
   "metadata": {},
   "source": [
    "# 2번 - 코랩으로 한 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b983a-213c-45c6-996a-144e4ce14543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.python.keras.backend import relu, sigmoid\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from ReplayMemory import ReplayMemory\n",
    "import numpy as np\n",
    "from ReplayMemory import ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819feb2-da8e-4401-9d38-405f95856d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, batch_size=100, gamma=0.9):\n",
    "        # 학습에 사용할 model과 target_model을 설정한다\n",
    "        self.model = self._create_model()\n",
    "        self.target_model = self._create_model()\n",
    "        # 처음에는 두 모델을 동일 weight로 설정해준다\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.replayMemory = ReplayMemory()\n",
    "        self.gamma = gamma  # 감마, 클수록 미래의 이익을 고려한다\n",
    "        self.batch_size = batch_size\n",
    "        self.callbacks = [\n",
    "            keras.callbacks.TensorBoard(\n",
    "                log_dir=\"my_log_dir\",\n",
    "                histogram_freq=1,\n",
    "                embeddings_freq=1,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _create_model(self) -> Sequential:\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(10, activation=relu, input_shape=(4,)))\n",
    "        model.add(layers.Dense(10, activation=relu))\n",
    "        model.add(layers.Dense(2))\n",
    "        model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "        return model\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model.predict(data)\n",
    "\n",
    "    # replayMemory를 이용해 Agent를 학습\n",
    "    def train(self):\n",
    "\n",
    "        # replayMemory에 저장된 experience의 개수는 2000개 이상이어야 함\n",
    "        if 2000 > len(self.replayMemory):\n",
    "            return\n",
    "\n",
    "        # batch_size만큼 샘플링한다\n",
    "        # (cur_state, action, reward, done, info, next_state) : list\n",
    "        samples = self.replayMemory.sample(self.batch_size)\n",
    "        # batch data를 생성한다\n",
    "\n",
    "        current_states = np.stack([sample[0] for sample in samples])\n",
    "        current_q = self.model.predict(current_states)\n",
    "        next_states = np.stack([sample[5] for sample in samples])\n",
    "        next_q = self.target_model.predict(next_states)\n",
    "\n",
    "        for i, (cur_state, action, reward, done, info, next_state) in enumerate(\n",
    "            samples\n",
    "        ):\n",
    "            if done:\n",
    "                next_q_value = reward\n",
    "            else:\n",
    "                next_q_value = reward + self.gamma * np.max(next_q[i])\n",
    "            current_q[i][action] = next_q_value\n",
    "\n",
    "        # 학습!!\n",
    "        self.model.fit(\n",
    "            x=current_states,\n",
    "            y=current_q,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    # target model의 가중치를 model의 가중치로 update 한다\n",
    "    def _update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        path: str,\n",
    "        model_name: str,\n",
    "        version: str,\n",
    "        num_trained: int,\n",
    "        target_model_name: str = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        모델 저장 이름 예시\n",
    "        /path/cartpole_v5_300_trained.h5\n",
    "\n",
    "        \"\"\"\n",
    "        save_name = f\"{path}/{model_name}_{version}_{num_trained}_trained.h5\"\n",
    "        target_model_name = f\"target_{model_name}\"\n",
    "        target_save_name = (\n",
    "            f\"{path}/{target_model_name}_{version}_{num_trained}_trained.h5\"\n",
    "        )\n",
    "        self.model.save(save_name)\n",
    "        self.target_model.save(target_save_name)\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        path: str,\n",
    "        model_name: str,\n",
    "        version: str,\n",
    "        num_trained: int,\n",
    "        target_model_name: str = None,\n",
    "    ):\n",
    "        save_name = f\"{path}/{model_name}_{version}_{num_trained}_trained.h5\"\n",
    "        target_model_name = f\"target_{model_name}\"\n",
    "        target_save_name = (\n",
    "            f\"{path}/{target_model_name}_{version}_{num_trained}_trained.h5\"\n",
    "        )\n",
    "        self.model = keras.models.load_model(save_name)\n",
    "        self.target_model = keras.models.load_model(target_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc414bc-1b48-4bde-841c-126fb3a04919",
   "metadata": {},
   "source": [
    "# DQNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f48b28-7ef0-4f4b-b4b7-04ea87e9dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import version\n",
    "from DQNAgent import DQNAgent\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d623d-e3a4-468e-a087-670215afe644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        max_episode=500,\n",
    "        step_size=2000,\n",
    "        epsilon=0.99,\n",
    "        epsilon_decay=0.995,\n",
    "        temp_save_freq=100,\n",
    "        model_path=os.path.join(os.getcwd(), \"model\"),\n",
    "        model_name=\"model\",\n",
    "        version=\"test\",\n",
    "        min_epsilon=0.01,\n",
    "        temp_save=True,\n",
    "        save_on_colab=False,\n",
    "    ):\n",
    "        self.agent = DQNAgent()\n",
    "        self.max_episode = max_episode\n",
    "        self.env = env\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        # 학습할때 임시로 저장할 빈도\n",
    "        self.temp_save_freq = temp_save_freq\n",
    "        # 모델을 저장할 경로\n",
    "        self.model_path = model_path\n",
    "        # 저장할 모델의 경로\n",
    "        self.model_name = model_name\n",
    "        # 저장할 최종 모델의 버전\n",
    "        self.version = version\n",
    "        self.target_model_path = \"target_\" + model_path\n",
    "        # epsilon greedy\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        # 임시 저장 여부\n",
    "        self.temp_save = temp_save\n",
    "        # colab에서 저장할지 여부\n",
    "        self.save_on_colab = save_on_colab\n",
    "        # episode별로 sum of reward를 저장\n",
    "        self.save_epi_reward = []\n",
    "        self.save_epi_step_num = []\n",
    "\n",
    "    def train(self):\n",
    "        pbar = tqdm(initial=0, total=self.max_episode, unit=\"episodes\")\n",
    "\n",
    "        for episode in range(self.max_episode):\n",
    "            cur_state = self.env.reset()\n",
    "            step, episode_reward, done = 0, 0, False\n",
    "\n",
    "            while not done:\n",
    "                if (np.random.randn(1)) <= self.epsilon:\n",
    "                    # 0, 1 중에서 무작위로 수를 하나 뽑는다\n",
    "                    action = np.random.randint(2)\n",
    "                else:\n",
    "                    # Q(cur_state,a)중에서 가장 값이 높도록 하는 a를 action으로 고른다\n",
    "                    output = self.agent.forward(cur_state.reshape(-1, 4))\n",
    "                    output = np.argmax(output)\n",
    "                    action = output\n",
    "\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                # replayMemory에 결과를 저장한다\n",
    "                self.agent.replayMemory.add(\n",
    "                    (cur_state, action, reward, done, info, next_state)\n",
    "                )\n",
    "\n",
    "                # replayMemory를 이용해 학습을 진행한다\n",
    "                self.agent.train()\n",
    "\n",
    "                # 상태 업데이트\n",
    "                cur_state = next_state\n",
    "                episode_reward += reward\n",
    "                step += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # target_model의 가중치를 model과 동기화\n",
    "            self.agent._update_target_model()\n",
    "\n",
    "            # epsilon decay\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.min_epsilon)\n",
    "\n",
    "            # 설정한 빈도에 따라서 임시 저장\n",
    "            if self.temp_save == True:\n",
    "                if (episode % self.temp_save_freq) == 0:\n",
    "                    if self.save_on_colab:\n",
    "                        self.colab_save(\n",
    "                            model_name=self.model_name,\n",
    "                            version=self.version,\n",
    "                            num_trained=episode,\n",
    "                        )\n",
    "                    else:\n",
    "                        self.agent.save(\n",
    "                            path=self.model_path,\n",
    "                            model_name=self.model_name,\n",
    "                            version=self.version,\n",
    "                            num_trained=episode,\n",
    "                        )\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "            self.save_epi_reward.append(episode_reward)\n",
    "            self.save_epi_step_num.append(step)\n",
    "\n",
    "            ####### 한 EPISODE 종료 #########\n",
    "\n",
    "        # --------------모든 에피소드 종료---------------- #\n",
    "\n",
    "        # 모든 학습이 끝나면 모델을 저장한다\n",
    "        if self.save_on_colab:\n",
    "            self.colab_save(\n",
    "                model_name=self.model_name,\n",
    "                version=self.version,\n",
    "                num_trained=self.max_episode,\n",
    "            )\n",
    "        else:\n",
    "            self.agent.save(\n",
    "                path=self.model_path,\n",
    "                model_name=self.model_name,\n",
    "                version=self.version,\n",
    "                num_trained=self.max_episode,\n",
    "            )\n",
    "\n",
    "        # episode에 따른 학습결과 (reward의 총합)을 그래프로 표시한다.\n",
    "        plt.plot(self.save_epi_reward)\n",
    "\n",
    "    def colab_save(self, model_name: str, version: str, num_trained: int):\n",
    "        from google.colab import drive\n",
    "        import os\n",
    "\n",
    "        mount_path = \"/content/drive\"\n",
    "        drive.mount(mount_path)\n",
    "\n",
    "        model_path = os.path.join(mount_path, \"MyDrive\", \"model\")\n",
    "\n",
    "        # save model on google drive\n",
    "        self.agent.save(\n",
    "            path=model_path,\n",
    "            model_name=model_name,\n",
    "            version=version,\n",
    "            num_trained=num_trained,\n",
    "        )\n",
    "\n",
    "    def colab_load(self, model_name: str, version: str, num_trained: int):\n",
    "        from google.colab import drive\n",
    "        import os\n",
    "\n",
    "        mount_path = \"/content/drive\"\n",
    "        drive.mount(mount_path)\n",
    "\n",
    "        model_path = os.path.join(mount_path, \"MyDrive\", \"model\")\n",
    "\n",
    "        # load model on google drive\n",
    "        self.agent.load(\n",
    "            path=model_path,\n",
    "            model_name=model_name,\n",
    "            version=version,\n",
    "            num_trained=num_trained,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6ca81-4038-478d-aff8-1365f936ff78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445261c4-e7c9-43ad-834c-fa8d71f6131a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3fdfe-bfab-4842-be46-be605d4e68a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
