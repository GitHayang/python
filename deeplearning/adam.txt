Optimization
딥러닝 학습시 최대한 틀리지 않는 방향으로 학습해야 한다,

얼마나 틀리는지(loss)를 알게 하는 함수가 loss function(손실함수)이다.
loss function 의 최솟값을 찾는 것을 학습 목표로 한다.

최소값을 찾아가는 것 최적화 = Optimization

이를 수행하는 알고리즘이 최적화 알고리즘 = Optimizer 이다.

학습속도를 빠르고 안정적이게하는 것을 목표로 한다.

아래 이미지는 여러 옵티마이저들이 어떻게 오차의 최저점을 찾아가는지 그래프로 형상화한 것이다.


Adam (Adaptive Moment Esimation)
Momentum 와 RMSProp 두가지를 섞어 쓴 알고리즘이다.

즉, 진행하던 속도에 관성을 주고, 최근 경로의 곡면의 변화량에 따른 적응적 학습률을 갖은 알고리즘이다.

매우 넓은 범위의 아키덱처를 가진 서로 다른 신경망에서 잘 작동한다는 것이 증명되어,
일반적 알고리즘에 현재 가장 많이 사용되고 있다.

아담의 강점은 bounded step size 이다.

Momentum 방식과 유사하게 지금까지 계산해온 기울기의 지수 평균을 저장하며,

RMSProp과 유사하게 기울기의 제곱값에 지수평균을 저장한다.



Adam 에서는 기울기 값과 기울기의 제곱값의 지수이동편균을 활용하여 step 변화량을 조절한다.

또한, 초기 몇번의 update 에서 0으로 편향되어 출발 지점에서 멀리 떨어진 곳으로 이동하는, 초기 경로의 편향 문제가 있는 RMSProp의 단점을 보정하는 매커니즘이 반영되어있다.
( m 과 v가 처음에 0으로 초기화되어있기 떄문에, 학습의 초반부에서는 mt, vt, mtxVt 가 0에 가깝게 bias 되있을 것이라고 판단하여, 이를 unbiased 하게 만들어주는 작업을 거친다.)

보정을 통해 unbiased 된 expectation을 얻을 수 있다.